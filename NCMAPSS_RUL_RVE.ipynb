{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c830f8c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-12 20:08:43.512915: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n"
     ]
    }
   ],
   "source": [
    "import utils\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import Input, Dense, Lambda, LSTM, RepeatVector, Bidirectional, Masking, Dropout\n",
    "from glob import glob\n",
    "import os\n",
    "import h5py\n",
    "import time\n",
    "import matplotlib\n",
    "import matplotlib.ticker as ticker\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from pandas import DataFrame\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import gridspec\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "%matplotlib inline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.pipeline import Pipeline\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from sklearn.model_selection import GroupShuffleSplit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94bda542",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"dataset/N-CMAPSS_DS01-005.h5\"\n",
    "df_X_cols = ['alt', 'Mach', 'TRA', 'T2', 'T24', 'T30', 'T48', 'T50', 'P15', 'P2',\n",
    "'P21', 'P24', 'Ps30', 'P40', 'P50', 'Nf', 'Nc', 'Wf', 'unit', 'cycle']\n",
    "\n",
    "df_y_cols = ['fan_eff_mod', 'fan_flow_mod', 'LPC_eff_mod', 'LPC_flow_mod',\n",
    "       'HPC_eff_mod', 'HPC_flow_mod', 'HPT_eff_mod', 'HPT_flow_mod',\n",
    "       'LPT_eff_mod', 'LPT_flow_mod', 'unit', 'alt', 'cycle']\n",
    "\n",
    "X = []\n",
    "# y = []\n",
    "\n",
    "df_X = pd.DataFrame(X,columns=df_X_cols)\n",
    "\n",
    "# Load data\n",
    "with h5py.File(filename, 'r') as hdf:\n",
    "    # Development set\n",
    "    W_dev = np.array(hdf.get('W_dev'))             # W\n",
    "    X_s_dev = np.array(hdf.get('X_s_dev'))         # X_s\n",
    "#         X_v_dev = np.array(hdf.get('X_v_dev'))         # X_v\n",
    "#         T_dev = np.array(hdf.get('T_dev'))             # T\n",
    "    Y_dev = np.array(hdf.get('Y_dev'))             # RUL  \n",
    "    A_dev = np.array(hdf.get('A_dev'))             # Auxiliary\n",
    "\n",
    "    # Test set\n",
    "    W_test = np.array(hdf.get('W_test'))           # W\n",
    "    X_s_test = np.array(hdf.get('X_s_test'))       # X_s\n",
    "#         X_v_test = np.array(hdf.get('X_v_test'))       # X_v\n",
    "#         T_test = np.array(hdf.get('T_test'))           # T\n",
    "    Y_test = np.array(hdf.get('Y_test'))           # RUL  \n",
    "    A_test = np.array(hdf.get('A_test'))           # Auxiliary\n",
    "\n",
    "    # Varnams\n",
    "    W_var = np.array(hdf.get('W_var'))\n",
    "    X_s_var = np.array(hdf.get('X_s_var'))  \n",
    "#         X_v_var = np.array(hdf.get('X_v_var')) \n",
    "#         T_var = np.array(hdf.get('T_var'))\n",
    "    A_var = np.array(hdf.get('A_var'))\n",
    "\n",
    "    # from np.array to list dtype U4/U5\n",
    "    W_var = list(np.array(W_var, dtype='U20'))\n",
    "    X_s_var = list(np.array(X_s_var, dtype='U20'))  \n",
    "#         X_v_var = list(np.array(X_v_var, dtype='U20')) \n",
    "#         T_var = list(np.array(T_var, dtype='U20'))\n",
    "    A_var = list(np.array(A_var, dtype='U20'))\n",
    "    W_Xs_var = W_var + X_s_var\n",
    "W_Xs_dev = np.concatenate((W_dev, X_s_dev), axis=1)\n",
    "W_Xs_test = np.concatenate((W_test, X_s_test), axis=1)\n",
    "\n",
    "df_A_dev = DataFrame(data=A_dev, columns=A_var)\n",
    "df_A_test = DataFrame(data=A_test, columns=A_var)\n",
    "\n",
    "df_W_Xs_dev = DataFrame(data=W_Xs_dev, columns=W_Xs_var)\n",
    "df_W_Xs_test = DataFrame(data=W_Xs_test, columns=W_Xs_var)\n",
    "#     df_T = DataFrame(data=T, columns=T_var)\n",
    "\n",
    "# Add the column \"unit\" to all dataframes\n",
    "#     df_T['unit'] = df_A['unit'].values\n",
    "#     df_T['alt'] = df_W_Xs['alt'].values\n",
    "df_W_Xs_dev['unit'] = df_A_dev['unit'].values\n",
    "df_W_Xs_test['unit'] = df_A_test['unit'].values\n",
    "\n",
    "#     df_T['cycle'] = df_A['cycle'].values\n",
    "df_W_Xs_dev['cycle'] = df_A_dev['cycle'].values\n",
    "df_W_Xs_test['cycle'] = df_A_test['cycle'].values\n",
    "\n",
    "df_W_Xs_dev['Fc'] = df_A_dev['Fc'].values\n",
    "df_W_Xs_test['Fc'] = df_A_test['Fc'].values\n",
    "#     df_W_Xs['HS'] = df_A['hs'].values\n",
    "\n",
    "cols = ['unit', 'cycle', 'Fc', 'alt', 'Mach',  'TRA', 'T2', 'T24', 'T30', 'T48', 'T50', 'P15', 'P2', 'P21', 'P24', 'Ps30', 'P40', 'P50', 'Nf', 'Nc', 'Wf', ]\n",
    "df_W_Xs_dev = df_W_Xs_dev[cols]\n",
    "df_W_Xs_test = df_W_Xs_test[cols]\n",
    "\n",
    "for col in df_W_Xs_dev:\n",
    "    if col in ['unit', 'cycle', 'alt', 'Fc']:\n",
    "        df_W_Xs_dev[col] = df_W_Xs_dev[col].astype(int)\n",
    "        df_W_Xs_test[col] = df_W_Xs_test[col].astype(int)\n",
    "    else:\n",
    "        df_W_Xs_dev[col] = df_W_Xs_dev[col].astype(float)\n",
    "        df_W_Xs_test[col] = df_W_Xs_test[col].astype(float)\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee723505",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_W_Xs_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f634a377",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_W_Xs_dev.shape, Y_dev.shape)\n",
    "print(df_W_Xs_test.shape, Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "848926f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_y_dev = pd.DataFrame(zip(df_W_Xs_dev.unit, Y_dev.reshape(len(Y_dev))), columns=['unit', 'RUL'])\n",
    "df_y_test = pd.DataFrame(zip(df_W_Xs_test.unit, Y_test.reshape(len(Y_test))), columns=['unit', 'RUL'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6830a22c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_operating_condition(df):\n",
    "    df_op_cond = df.copy()\n",
    "    # 'Fc', 'alt', 'Mach',  'TRA'\n",
    "    df_op_cond['TRA'] = abs(df_op_cond['TRA'].round()).astype(int)\n",
    "#     df_op_cond['Mach'] = abs(df_op_cond['Mach'].round(decimals=2))\n",
    "\n",
    "    # converting settings to string and concatanating makes the operating condition into a categorical variable\n",
    "    df_op_cond['op_cond'] = df_op_cond['Fc'].astype(str) + '_' + \\\n",
    "                            df_op_cond['Mach'].round(decimals=1).astype(str) + '_' + \\\n",
    "                            df_op_cond['TRA'].astype(str)\n",
    "#     (df_op_cond['alt']//1000).astype(str) + '_' + \\\n",
    "    df_op_cond['op_cond'] = df_op_cond.op_cond.astype(str)\n",
    "\n",
    "    return df_op_cond\n",
    "\n",
    "def condition_scaler(df, sensor_names):\n",
    "    # apply operating condition specific scaling\n",
    "    scaler = StandardScaler()\n",
    "    for condition in df['op_cond'].unique():\n",
    "        print(list(df['op_cond'].unique()).index(condition))\n",
    "#         scaler.fit(df.loc[df['op_cond'] == condition, sensor_names])\n",
    "        df.loc[df['op_cond'] == condition, sensor_names] = scaler.fit_transform(df.loc[df['op_cond'] == condition, sensor_names])\n",
    "    return df\n",
    "\n",
    "def exponential_smoothing(df, sensors, n_samples, alpha=0.4):\n",
    "    df = df.copy()\n",
    "    # first, take the exponential weighted mean\n",
    "    df[sensors] = df.groupby('unit')[sensors].apply(lambda x: x.ewm(alpha=alpha).mean()).reset_index(level=0, drop=True)\n",
    "\n",
    "    # second, drop first n_samples of each unit_nr to reduce filter delay\n",
    "    def create_mask(data, samples):\n",
    "        result = np.ones_like(data)\n",
    "        result[0:samples] = 0\n",
    "        return result\n",
    "\n",
    "    mask = df.groupby('unit')['unit'].transform(create_mask, samples=n_samples).astype(bool)\n",
    "    df = df[mask]\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82d3558a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sensor_names = ['T2', 'T24', 'T30', 'T48', 'T50', 'P15', 'P2', 'P21', 'P24', 'Ps30', 'P40', 'P50', 'Nf', 'Nc', 'Wf',]\n",
    "sensors = ['T2', 'T24', 'T30', 'T48', 'T50', 'P15', 'P2', 'P21', 'P24', 'Ps30', 'P40', 'P50', 'Nf', 'Nc', 'Wf',]\n",
    "# remove unused sensors\n",
    "drop_sensors = [element for element in sensor_names if element not in sensors]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb996dd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_sensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "383d59b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_pre = add_operating_condition(df_W_Xs_dev.drop(drop_sensors, axis=1))\n",
    "X_test_pre = add_operating_condition(df_W_Xs_test.drop(drop_sensors, axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd309076",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_train_pre.op_cond.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f43b22ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(X_train_pre.op_cond.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e74644b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "X_train_pre.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf837271",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_train_pre = condition_scaler(X_train_pre, sensors)\n",
    "X_test_pre = condition_scaler(X_test_pre, sensors)\n",
    "print(X_train_pre.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a563fef7",
   "metadata": {},
   "source": [
    "X_tr_pre_copy = X_train_pre.copy()\n",
    "X_te_pre_copy = X_test_pre.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "649f5842",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_pre = exponential_smoothing(X_train_pre, sensors, 0, alpha=0.1)\n",
    "X_test_pre = exponential_smoothing(X_test_pre, sensors, 0, alpha=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56aee526",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_train_pre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e86e25f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_train_data(df, sequence_length, columns):\n",
    "    data = df[columns].values\n",
    "    num_elements = data.shape[0]\n",
    "\n",
    "    # -1 and +1 because of Python indexing\n",
    "    for start, stop in zip(range(0, num_elements - (sequence_length - 1)), range(sequence_length, num_elements + 1)):\n",
    "        yield data[start:stop, :]\n",
    "\n",
    "\n",
    "def gen_data_wrapper(df, sequence_length, columns, unit_nrs=np.array([])):\n",
    "    if unit_nrs.size <= 0:\n",
    "        unit_nrs = df['unit'].unique()\n",
    "\n",
    "    data_gen = (list(gen_train_data(df[df['unit'] == unit_nr], sequence_length, columns))\n",
    "                for unit_nr in unit_nrs)\n",
    "    data_array = np.concatenate(list(data_gen)).astype(np.float32)\n",
    "    return data_array\n",
    "\n",
    "\n",
    "def gen_labels(df, sequence_length, label):\n",
    "    data_matrix = df[label].values\n",
    "    num_elements = data_matrix.shape[0]\n",
    "\n",
    "    # -1 because I want to predict the rul of that last row in the sequence, not the next row\n",
    "    return data_matrix[sequence_length - 1:num_elements, :]\n",
    "\n",
    "\n",
    "def gen_label_wrapper(df, sequence_length, label, unit_nrs=np.array([])):\n",
    "    if unit_nrs.size <= 0:\n",
    "        unit_nrs = df['unit'].unique()\n",
    "\n",
    "    label_gen = [gen_labels(df[df['unit'] == unit_nr], sequence_length, label)\n",
    "                 for unit_nr in unit_nrs]\n",
    "    label_array = np.concatenate(label_gen).astype(np.float32)\n",
    "    return label_array\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "802d751b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_length = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36ad83b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "gss = GroupShuffleSplit(n_splits=1, train_size=0.87, random_state=42)\n",
    "for train_unit, val_unit in gss.split(X_train_pre['unit'].unique(), groups=X_train_pre['unit'].unique()):\n",
    "    print(train_unit, val_unit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88d63cd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train-val split\n",
    "gss = GroupShuffleSplit(n_splits=1, train_size=0.87, random_state=42)\n",
    "# generate the train/val for *each* sample -> for that we iterate over the train and val units we want\n",
    "# this is a for that iterates only once and in that iterations at the same time iterates over all the values we want,\n",
    "# i.e. train_unit and val_unit are not a single value but a set of training/vali units\n",
    "for train_unit, val_unit in gss.split(X_train_pre['unit'].unique(), groups=X_train_pre['unit'].unique()):\n",
    "    train_unit = X_train_pre['unit'].unique()[train_unit]  # gss returns indexes and index starts at 1\n",
    "    val_unit = X_train_pre['unit'].unique()[val_unit]\n",
    "\n",
    "    x_train = gen_data_wrapper(X_train_pre, sequence_length, sensors, train_unit)\n",
    "    y_train = gen_label_wrapper(df_y_dev, sequence_length, ['RUL'], train_unit)\n",
    "\n",
    "    x_val = gen_data_wrapper(X_train_pre, sequence_length, sensors, val_unit)\n",
    "    y_val = gen_label_wrapper(df_y_dev, sequence_length, ['RUL'], val_unit)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c88ae565",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_test_data(df, sequence_length, columns, mask_value):\n",
    "    if df.shape[0] < sequence_length:\n",
    "        data_matrix = np.full(shape=(sequence_length, len(columns)), fill_value=mask_value)  # pad\n",
    "        idx = data_matrix.shape[0] - df.shape[0]\n",
    "        data_matrix[idx:, :] = df[columns].values  # fill with available data\n",
    "    else:\n",
    "        data_matrix = df[columns].values\n",
    "\n",
    "    # specifically yield the last possible sequence\n",
    "    stop = data_matrix.shape[0]\n",
    "    start = stop - sequence_length\n",
    "    for i in list(range(1)):\n",
    "        yield data_matrix[start:stop, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "125bffae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create sequences for test\n",
    "test_gen = (list(gen_test_data(X_test_pre[X_test_pre['unit'] == unit_nr], sequence_length, sensors, -99.))\n",
    "            for unit_nr in X_test_pre['unit'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9354d74f",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = np.concatenate(list(test_gen)).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bc42959",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train, x_val, y_val, x_test, df_y_test.RUL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "737b08e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edcdf04f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cba1f04e",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bd319bc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f83a96ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1300ecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f77b398a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_y_test.RUL.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dda6f18",
   "metadata": {},
   "source": [
    "os.mkdir('./RULRVE_numpydata')\n",
    "\n",
    "np.save('RULRVE_numpydata/x_train.npy', x_train)\n",
    "np.save('RULRVE_numpydata/x_val.npy', x_val)\n",
    "np.save('RULRVE_numpydata/y_train.npy', y_train)\n",
    "np.save('RULRVE_numpydata/y_val.npy', y_val)\n",
    "np.save('RULRVE_numpydata/x_test.npy', x_test)\n",
    "\n",
    "df_y_test.to_csv('RULRVE_numpydata/df_y_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "873361ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from os.path import join as opj\n",
    "def loader(filepath):\n",
    "    with open(filepath, 'rb') as fp:\n",
    "        np_ndarray = np.load(fp)\n",
    "    print(\"{} has a numpy ndarray of shape: {}\".format(filepath, np_ndarray.shape))\n",
    "    return np_ndarray\n",
    "\n",
    "data_dir = '/data/adas_vision_data1/users/adithya/turbofan_ncmapss/RULRVE_numpydata'\n",
    "x_train = loader(opj(data_dir, 'x_train.npy'))\n",
    "x_val = loader(opj(data_dir, 'x_val.npy'))\n",
    "y_train = loader(opj(data_dir, 'y_train.npy'))\n",
    "y_val = loader(opj(data_dir, 'y_val.npy'))\n",
    "# x_val = x_val[:300000, ...]\n",
    "# y_val = y_val[:300000, ...]\n",
    "x_test = loader(opj(data_dir, 'x_test.npy'))\n",
    "df_y_test = pd.read_csv(opj(data_dir, 'df_y_test.csv'))\n",
    "y_test = df_y_test.RUL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7be4a323",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e423f11",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.mkdir('./RULRVE_numpydata_1_val_eng')\n",
    "\n",
    "np.save('RULRVE_numpydata_1_val_eng/x_train.npy', x_train)\n",
    "np.save('RULRVE_numpydata_1_val_eng/x_val.npy', x_val)\n",
    "np.save('RULRVE_numpydata_1_val_eng/y_train.npy', y_train)\n",
    "np.save('RULRVE_numpydata_1_val_eng/y_val.npy', y_val)\n",
    "np.save('RULRVE_numpydata_1_val_eng/x_test.npy', x_test)\n",
    "\n",
    "df_y_test.to_csv('RULRVE_numpydata_1_val_eng/df_y_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4ac5c9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4e0bfec",
   "metadata": {},
   "outputs": [],
   "source": [
    "66: 4369/4369 - 184s - loss: 13.2013 - kl_loss: 3.5793 - reg_loss: 9.6220 - val_loss: 12.9760 - val_kl_loss: 4.5276 - val_reg_loss: 8.4484 - 184s/epoch - 42ms/step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b140e8e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/home/a0484689/PycharmProjects/Remaining-Useful-Life-Estimation-Variational')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "912a2008",
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils\n",
    "import model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92ef89b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------- MODEL -----------------------------------\n",
    "# no_of_samples = x_train.shape[0]\n",
    "timesteps = x_train.shape[1]\n",
    "input_dim = x_train.shape[2]\n",
    "intermediate_dim = 300\n",
    "batch_size = 128\n",
    "latent_dim = 2\n",
    "epochs = 10000\n",
    "optimizer = 'adam'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f872365",
   "metadata": {},
   "outputs": [],
   "source": [
    "RVE = model.create_model(timesteps, input_dim, intermediate_dim, \n",
    "                         batch_size, latent_dim, epochs, optimizer,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99cbbeb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Callbacks for training\n",
    "model_callbacks = utils.get_callbacks(RVE, x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c710dce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------- TRAINING ---------------------------------\n",
    "results = RVE.fit(x_train, y_train, shuffle=True, epochs=epochs,\n",
    "                  batch_size=batch_size, validation_data= (x_val, y_val),\n",
    "                  callbacks=model_callbacks, verbose=2)\n",
    "# -----------------------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a787f468",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(filename):\n",
    "    df_X_cols = ['alt', 'Mach', 'TRA', 'T2', 'T24', 'T30', 'T48', 'T50', 'P15', 'P2',\n",
    "                 'P21', 'P24', 'Ps30', 'P40', 'P50', 'Nf', 'Nc', 'Wf', 'unit', 'cycle']\n",
    "\n",
    "    df_y_cols = ['fan_eff_mod', 'fan_flow_mod', 'LPC_eff_mod', 'LPC_flow_mod',\n",
    "                 'HPC_eff_mod', 'HPC_flow_mod', 'HPT_eff_mod', 'HPT_flow_mod',\n",
    "                 'LPT_eff_mod', 'LPT_flow_mod', 'unit', 'alt', 'cycle']\n",
    "\n",
    "    X = []\n",
    "    # y = []\n",
    "\n",
    "    df_X = pd.DataFrame(X, columns=df_X_cols)\n",
    "\n",
    "    # Load data\n",
    "    with h5py.File(filename, 'r') as hdf:\n",
    "        # Development set\n",
    "        W_dev = np.array(hdf.get('W_dev'))  # W\n",
    "        X_s_dev = np.array(hdf.get('X_s_dev'))  # X_s\n",
    "        #         X_v_dev = np.array(hdf.get('X_v_dev'))         # X_v\n",
    "        #         T_dev = np.array(hdf.get('T_dev'))             # T\n",
    "        Y_dev = np.array(hdf.get('Y_dev'))  # RUL\n",
    "        A_dev = np.array(hdf.get('A_dev'))  # Auxiliary\n",
    "\n",
    "        # Test set\n",
    "        W_test = np.array(hdf.get('W_test'))  # W\n",
    "        X_s_test = np.array(hdf.get('X_s_test'))  # X_s\n",
    "        #         X_v_test = np.array(hdf.get('X_v_test'))       # X_v\n",
    "        #         T_test = np.array(hdf.get('T_test'))           # T\n",
    "        Y_test = np.array(hdf.get('Y_test'))  # RUL\n",
    "        A_test = np.array(hdf.get('A_test'))  # Auxiliary\n",
    "\n",
    "        # Varnams\n",
    "        W_var = np.array(hdf.get('W_var'))\n",
    "        X_s_var = np.array(hdf.get('X_s_var'))\n",
    "        #         X_v_var = np.array(hdf.get('X_v_var'))\n",
    "        #         T_var = np.array(hdf.get('T_var'))\n",
    "        A_var = np.array(hdf.get('A_var'))\n",
    "\n",
    "        # from np.array to list dtype U4/U5\n",
    "        W_var = list(np.array(W_var, dtype='U20'))\n",
    "        X_s_var = list(np.array(X_s_var, dtype='U20'))\n",
    "        #         X_v_var = list(np.array(X_v_var, dtype='U20'))\n",
    "        #         T_var = list(np.array(T_var, dtype='U20'))\n",
    "        A_var = list(np.array(A_var, dtype='U20'))\n",
    "        W_Xs_var = W_var + X_s_var\n",
    "    W_Xs_dev = np.concatenate((W_dev, X_s_dev), axis=1)\n",
    "    W_Xs_test = np.concatenate((W_test, X_s_test), axis=1)\n",
    "\n",
    "    df_A_dev = DataFrame(data=A_dev, columns=A_var)\n",
    "    df_A_test = DataFrame(data=A_test, columns=A_var)\n",
    "\n",
    "    df_W_Xs_dev = DataFrame(data=W_Xs_dev, columns=W_Xs_var)\n",
    "    df_W_Xs_test = DataFrame(data=W_Xs_test, columns=W_Xs_var)\n",
    "    #     df_T = DataFrame(data=T, columns=T_var)\n",
    "\n",
    "    # Add the column \"unit\" to all dataframes\n",
    "    #     df_T['unit'] = df_A['unit'].values\n",
    "    #     df_T['alt'] = df_W_Xs['alt'].values\n",
    "    df_W_Xs_dev['unit'] = df_A_dev['unit'].values\n",
    "    df_W_Xs_test['unit'] = df_A_test['unit'].values\n",
    "\n",
    "    #     df_T['cycle'] = df_A['cycle'].values\n",
    "    df_W_Xs_dev['cycle'] = df_A_dev['cycle'].values\n",
    "    df_W_Xs_test['cycle'] = df_A_test['cycle'].values\n",
    "\n",
    "    df_W_Xs_dev['Fc'] = df_A_dev['Fc'].values\n",
    "    df_W_Xs_test['Fc'] = df_A_test['Fc'].values\n",
    "    #     df_W_Xs['HS'] = df_A['hs'].values\n",
    "\n",
    "    cols = ['unit', 'cycle', 'Fc', 'alt', 'Mach', 'TRA', 'T2', 'T24', 'T30', 'T48', 'T50', 'P15', 'P2', 'P21', 'P24',\n",
    "            'Ps30', 'P40', 'P50', 'Nf', 'Nc', 'Wf', ]\n",
    "    df_W_Xs_dev = df_W_Xs_dev[cols]\n",
    "    df_W_Xs_test = df_W_Xs_test[cols]\n",
    "\n",
    "    for col in df_W_Xs_dev:\n",
    "        if col in ['unit', 'cycle', 'alt', 'Fc']:\n",
    "            df_W_Xs_dev[col] = df_W_Xs_dev[col].astype(int)\n",
    "            df_W_Xs_test[col] = df_W_Xs_test[col].astype(int)\n",
    "        else:\n",
    "            df_W_Xs_dev[col] = df_W_Xs_dev[col].astype(float)\n",
    "            df_W_Xs_test[col] = df_W_Xs_test[col].astype(float)\n",
    "\n",
    "    df_y_dev = pd.DataFrame(zip(df_W_Xs_dev.unit, Y_dev.reshape(len(Y_dev))), columns=['unit', 'RUL'])\n",
    "    df_y_test = pd.DataFrame(zip(df_W_Xs_test.unit, Y_test.reshape(len(Y_test))), columns=['unit', 'RUL'])\n",
    "\n",
    "    return df_W_Xs_dev, df_y_dev, df_W_Xs_test, df_y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "714ef8f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename1 = \"/data/adas_vision_data1/users/adithya/turbofan_ncmapss/dataset/N-CMAPSS_DS01-005.h5\"\n",
    "filename2 = \"/data/adas_vision_data1/users/adithya/turbofan_ncmapss/dataset/N-CMAPSS_DS02-006.h5\"\n",
    "filename3 = \"/data/adas_vision_data1/users/adithya/turbofan_ncmapss/dataset/N-CMAPSS_DS03-012.h5\"\n",
    "filename4 = \"/data/adas_vision_data1/users/adithya/turbofan_ncmapss/dataset/N-CMAPSS_DS04.h5\"\n",
    "filename5 = \"/data/adas_vision_data1/users/adithya/turbofan_ncmapss/dataset/N-CMAPSS_DS05.h5\"\n",
    "filename6 = \"/data/adas_vision_data1/users/adithya/turbofan_ncmapss/dataset/N-CMAPSS_DS06.h5\"\n",
    "filename7 = \"/data/adas_vision_data1/users/adithya/turbofan_ncmapss/dataset/N-CMAPSS_DS07.h5\"\n",
    "filename8 = \"/data/adas_vision_data1/users/adithya/turbofan_ncmapss/dataset/N-CMAPSS_DS08a-009.h5\"\n",
    "filename9 = \"/data/adas_vision_data1/users/adithya/turbofan_ncmapss/dataset/N-CMAPSS_DS08c-008.h5\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79b8a1bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_W_Xs_dev1, df_y_dev1, df_W_Xs_test1, df_y_test1 = load_data(filename1)\n",
    "# df_W_Xs_dev2, df_y_dev2, df_W_Xs_test2, df_y_test2 = load_data(filename2)\n",
    "# df_W_Xs_dev3, df_y_dev3, df_W_Xs_test3, df_y_test3 = load_data(filename3)\n",
    "# df_W_Xs_dev4, df_y_dev4, df_W_Xs_test4, df_y_test4 = load_data(filename4)\n",
    "# df_W_Xs_dev5, df_y_dev5, df_W_Xs_test5, df_y_test5 = load_data(filename5)\n",
    "# df_W_Xs_dev6, df_y_dev6, df_W_Xs_test6, df_y_test6 = load_data(filename6)\n",
    "# df_W_Xs_dev7, df_y_dev7, df_W_Xs_test7, df_y_test7 = load_data(filename7)\n",
    "# df_W_Xs_dev8, df_y_dev8, df_W_Xs_test8, df_y_test8 = load_data(filename8)\n",
    "# df_W_Xs_dev9, df_y_dev9, df_W_Xs_test9, df_y_test9 = load_data(filename9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f2cd3e7",
   "metadata": {},
   "source": [
    "df_W_Xs_dev1.to_pickle('df_W_Xs_dev1.pkl')\n",
    "df_y_dev1.to_pickle('df_y_dev1.pkl')\n",
    "df_W_Xs_test1.to_pickle('df_W_Xs_test1.pkl')\n",
    "df_y_test1.to_pickle('df_y_test1.pkl')\n",
    "\n",
    "df_W_Xs_dev2.to_pickle('df_W_Xs_dev2.pkl')\n",
    "df_y_dev2.to_pickle('df_y_dev2.pkl')\n",
    "df_W_Xs_test2.to_pickle('df_W_Xs_test2.pkl')\n",
    "df_y_test2.to_pickle('df_y_test2.pkl')\n",
    "\n",
    "df_W_Xs_dev3.to_pickle('df_W_Xs_dev3.pkl')\n",
    "df_y_dev3.to_pickle('df_y_dev3.pkl')\n",
    "df_W_Xs_test3.to_pickle('df_W_Xs_test3.pkl')\n",
    "df_y_test3.to_pickle('df_y_test3.pkl')\n",
    "\n",
    "df_W_Xs_dev4.to_pickle('df_W_Xs_dev4.pkl')\n",
    "df_y_dev4.to_pickle('df_y_dev4.pkl')\n",
    "df_W_Xs_test4.to_pickle('df_W_Xs_test4.pkl')\n",
    "df_y_test4.to_pickle('df_y_test4.pkl')\n",
    "\n",
    "df_W_Xs_dev5.to_pickle('df_W_Xs_dev5.pkl')\n",
    "df_y_dev5.to_pickle('df_y_dev5.pkl')\n",
    "df_W_Xs_test5.to_pickle('df_W_Xs_test5.pkl')\n",
    "df_y_test5.to_pickle('df_y_test5.pkl')\n",
    "\n",
    "df_W_Xs_dev6.to_pickle('df_W_Xs_dev6.pkl')\n",
    "df_y_dev6.to_pickle('df_y_dev6.pkl')\n",
    "df_W_Xs_test6.to_pickle('df_W_Xs_test6.pkl')\n",
    "df_y_test6.to_pickle('df_y_test6.pkl')\n",
    "\n",
    "df_W_Xs_dev7.to_pickle('df_W_Xs_dev7.pkl')\n",
    "df_y_dev7.to_pickle('df_y_dev7.pkl')\n",
    "df_W_Xs_test7.to_pickle('df_W_Xs_test7.pkl')\n",
    "df_y_test7.to_pickle('df_y_test7.pkl')\n",
    "\n",
    "df_W_Xs_dev8.to_pickle('df_W_Xs_dev8.pkl')\n",
    "df_y_dev8.to_pickle('df_y_dev8.pkl')\n",
    "df_W_Xs_test8.to_pickle('df_W_Xs_test8.pkl')\n",
    "df_y_test8.to_pickle('df_y_test8.pkl')\n",
    "\n",
    "df_W_Xs_dev9.to_pickle('df_W_Xs_dev9.pkl')\n",
    "df_y_dev9.to_pickle('df_y_dev9.pkl')\n",
    "df_W_Xs_test9.to_pickle('df_W_Xs_test9.pkl')\n",
    "df_y_test9.to_pickle('df_y_test9.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2de7e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_W_Xs_dev1 = pd.read_pickle('df_W_Xs_dev1.pkl')\n",
    "df_y_dev1 = pd.read_pickle('df_y_dev1.pkl')\n",
    "df_W_Xs_test1 = pd.read_pickle('df_W_Xs_test1.pkl')\n",
    "df_y_test1 = pd.read_pickle('df_y_test1.pkl')\n",
    "\n",
    "# df_W_Xs_dev2 = pd.read_pickle('df_W_Xs_dev2.pkl')\n",
    "# df_y_dev2 = pd.read_pickle('df_y_dev2.pkl')\n",
    "# df_W_Xs_test2 = pd.read_pickle('df_W_Xs_test2.pkl')\n",
    "# df_y_test2 = pd.read_pickle('df_y_test2.pkl')\n",
    "\n",
    "# df_W_Xs_dev3 = pd.read_pickle('df_W_Xs_dev3.pkl')\n",
    "# df_y_dev3 = pd.read_pickle('df_y_dev3.pkl')\n",
    "# df_W_Xs_test3 = pd.read_pickle('df_W_Xs_test3.pkl')\n",
    "# df_y_test3 = pd.read_pickle('df_y_test3.pkl')\n",
    "\n",
    "# df_W_Xs_dev4 = pd.read_pickle('df_W_Xs_dev4.pkl')\n",
    "# df_y_dev4 = pd.read_pickle('df_y_dev4.pkl')\n",
    "# df_W_Xs_test4 = pd.read_pickle('df_W_Xs_test4.pkl')\n",
    "# df_y_test4 = pd.read_pickle('df_y_test4.pkl')\n",
    "\n",
    "# df_W_Xs_dev5 = pd.read_pickle('df_W_Xs_dev5.pkl')\n",
    "# df_y_dev5 = pd.read_pickle('df_y_dev5.pkl')\n",
    "# df_W_Xs_test5 = pd.read_pickle('df_W_Xs_test5.pkl')\n",
    "# df_y_test5 = pd.read_pickle('df_y_test5.pkl')\n",
    "\n",
    "# df_W_Xs_dev6 = pd.read_pickle('df_W_Xs_dev6.pkl')\n",
    "# df_y_dev6 = pd.read_pickle('df_y_dev6.pkl')\n",
    "# df_W_Xs_test6 = pd.read_pickle('df_W_Xs_test6.pkl')\n",
    "# df_y_test6 = pd.read_pickle('df_y_test6.pkl')\n",
    "\n",
    "# df_W_Xs_dev7 = pd.read_pickle('df_W_Xs_dev7.pkl')\n",
    "# df_y_dev7 = pd.read_pickle('df_y_dev7.pkl')\n",
    "# df_W_Xs_test7 = pd.read_pickle('df_W_Xs_test7.pkl')\n",
    "# df_y_test7 = pd.read_pickle('df_y_test7.pkl')\n",
    "\n",
    "# df_W_Xs_dev8 = pd.read_pickle('df_W_Xs_dev8.pkl')\n",
    "# df_y_dev8 = pd.read_pickle('df_y_dev8.pkl')\n",
    "# df_W_Xs_test8 = pd.read_pickle('df_W_Xs_test8.pkl')\n",
    "# df_y_test8 = pd.read_pickle('df_y_test8.pkl')\n",
    "\n",
    "# df_W_Xs_dev9 = pd.read_pickle('df_W_Xs_dev9.pkl')\n",
    "# df_y_dev9 = pd.read_pickle('df_y_dev9.pkl')\n",
    "# df_W_Xs_test9 = pd.read_pickle('df_W_Xs_test9.pkl')\n",
    "# df_y_test9 = pd.read_pickle('df_y_test9.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab9e7cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_length = 30\n",
    "sensors = ['T2', 'T24', 'T30', 'T48', 'T50', 'P15', 'P2', 'P21', 'P24', 'Ps30', 'P40', 'P50', 'Nf', 'Nc', 'Wf', ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9726660a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(sequence_length, sensors, df_W_Xs_dev, df_y_dev, df_W_Xs_test, df_y_test):\n",
    "    # remove unused sensors\n",
    "    sensor_names = ['T2', 'T24', 'T30', 'T48', 'T50', 'P15', 'P2', 'P21', 'P24', 'Ps30', 'P40', 'P50', 'Nf', 'Nc',\n",
    "                    'Wf', ]\n",
    "    drop_sensors = [element for element in sensor_names if element not in sensors]\n",
    "\n",
    "    X_train_pre = add_operating_condition(df_W_Xs_dev.drop(drop_sensors, axis=1))\n",
    "    X_test_pre = add_operating_condition(df_W_Xs_test.drop(drop_sensors, axis=1))\n",
    "\n",
    "    X_train_pre = condition_scaler(X_train_pre, sensors)\n",
    "    X_test_pre = condition_scaler(X_test_pre, sensors)\n",
    "\n",
    "    X_train_pre = exponential_smoothing(X_train_pre, sensors, 0, alpha=0.1)\n",
    "    X_test_pre = exponential_smoothing(X_test_pre, sensors, 0, alpha=0.1)\n",
    "\n",
    "    # train-val split\n",
    "    gss = GroupShuffleSplit(n_splits=1, train_size=0.87, random_state=42)\n",
    "    # generate the train/val for *each* sample -> for that we iterate over the train and val units we want\n",
    "    # this is a for that iterates only once and in that iterations at the same time iterates over all the values we want,\n",
    "    # i.e. train_unit and val_unit are not a single value but a set of training/vali units\n",
    "    for train_unit, val_unit in gss.split(X_train_pre['unit'].unique(), groups=X_train_pre['unit'].unique()):\n",
    "        train_unit = X_train_pre['unit'].unique()[train_unit]  # gss returns indexes and index starts at 1\n",
    "        val_unit = X_train_pre['unit'].unique()[val_unit]\n",
    "\n",
    "        x_train = gen_data_wrapper(X_train_pre, sequence_length, sensors, train_unit)\n",
    "        y_train = gen_label_wrapper(df_y_dev, sequence_length, ['RUL'], train_unit)\n",
    "\n",
    "        x_val = gen_data_wrapper(X_train_pre, sequence_length, sensors, val_unit)\n",
    "        y_val = gen_label_wrapper(df_y_dev, sequence_length, ['RUL'], val_unit)\n",
    "\n",
    "#     # create sequences for test\n",
    "#     test_gen = (list(gen_test_data(X_test_pre[X_test_pre['unit'] == unit_nr], sequence_length, sensors, -99.))\n",
    "#                 for unit_nr in X_test_pre['unit'].unique())\n",
    "#     x_test = np.concatenate(list(test_gen)).astype(np.float32)\n",
    "    for test_unit in X_test_pre.unit.unique():\n",
    "        x_test = gen_data_wrapper(X_test_pre, sequence_length, sensors, test_unit)\n",
    "        y_test = gen_label_wrapper(df_y_test, sequence_length, ['RUL'], test_unit)\n",
    "\n",
    "    return x_train, y_train, x_val, y_val, x_test, y_test\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cd10c2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/home/a0484689/PycharmProjects/Remaining-Useful-Life-Estimation-Variational')\n",
    "from ncmapss_rul_rve import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fef3dece",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_W_Xs_test1.unit.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dad6243",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "This module is to regenerate x_test and y_test in the same format as train data.\n",
    "The previous module has already been updated\n",
    "\n",
    "'''\n",
    "def prepare_data(sequence_length, sensors, df_W_Xs_test, df_y_test):\n",
    "    # remove unused sensors\n",
    "    sensor_names = ['T2', 'T24', 'T30', 'T48', 'T50', 'P15', 'P2', 'P21', 'P24', 'Ps30', 'P40', 'P50', 'Nf', 'Nc',\n",
    "                    'Wf', ]\n",
    "    drop_sensors = [element for element in sensor_names if element not in sensors]\n",
    "\n",
    "#     X_train_pre = add_operating_condition(df_W_Xs_dev.drop(drop_sensors, axis=1))\n",
    "    X_test_pre = add_operating_condition(df_W_Xs_test.drop(drop_sensors, axis=1))\n",
    "\n",
    "#     X_train_pre = condition_scaler(X_train_pre, sensors)\n",
    "    X_test_pre = condition_scaler(X_test_pre, sensors)\n",
    "\n",
    "#     X_train_pre = exponential_smoothing(X_train_pre, sensors, 0, alpha=0.1)\n",
    "    X_test_pre = exponential_smoothing(X_test_pre, sensors, 0, alpha=0.1)\n",
    "\n",
    "    # train-val split\n",
    "#     gss = GroupShuffleSplit(n_splits=1, train_size=0.87, random_state=42)\n",
    "#     # generate the train/val for *each* sample -> for that we iterate over the train and val units we want\n",
    "#     # this is a for that iterates only once and in that iterations at the same time iterates over all the values we want,\n",
    "#     # i.e. train_unit and val_unit are not a single value but a set of training/vali units\n",
    "#     for train_unit, val_unit in gss.split(X_train_pre['unit'].unique(), groups=X_train_pre['unit'].unique()):\n",
    "#         train_unit = X_train_pre['unit'].unique()[train_unit]  # gss returns indexes and index starts at 1\n",
    "#         val_unit = X_train_pre['unit'].unique()[val_unit]\n",
    "\n",
    "#         x_train = gen_data_wrapper(X_train_pre, sequence_length, sensors, train_unit)\n",
    "#         y_train = gen_label_wrapper(df_y_dev, sequence_length, ['RUL'], train_unit)\n",
    "\n",
    "#         x_val = gen_data_wrapper(X_train_pre, sequence_length, sensors, val_unit)\n",
    "#         y_val = gen_label_wrapper(df_y_dev, sequence_length, ['RUL'], val_unit)\n",
    "\n",
    "#     # create sequences for test\n",
    "#     test_gen = (list(gen_test_data(X_test_pre[X_test_pre['unit'] == unit_nr], sequence_length, sensors, -99.))\n",
    "#                 for unit_nr in X_test_pre['unit'].unique())\n",
    "#     x_test = np.concatenate(list(test_gen)).astype(np.float32)\n",
    "#     for test_unit in X_test_pre.unit.unique():\n",
    "    x_test = gen_data_wrapper(X_test_pre, sequence_length, sensors, X_test_pre.unit.unique())\n",
    "    y_test = gen_label_wrapper(df_y_test, sequence_length, ['RUL'], X_test_pre.unit.unique())\n",
    "\n",
    "    return x_test, y_test\n",
    "\n",
    "sequence_length = 30\n",
    "sensors = ['T2', 'T24', 'T30', 'T48', 'T50', 'P15', 'P2', 'P21', 'P24', 'Ps30', 'P40', 'P50', 'Nf', 'Nc', 'Wf', ]\n",
    "\n",
    "'''Read from dataframe'''\n",
    "# df_W_Xs_test1 = pd.read_pickle('df_W_Xs_test1.pkl')\n",
    "# df_y_test1 = pd.read_pickle('df_y_test1.pkl')\n",
    "\n",
    "# df_W_Xs_test2 = pd.read_pickle('df_W_Xs_test2.pkl')\n",
    "# df_y_test2 = pd.read_pickle('df_y_test2.pkl')\n",
    "\n",
    "# df_W_Xs_test3 = pd.read_pickle('df_W_Xs_test3.pkl')\n",
    "# df_y_test3 = pd.read_pickle('df_y_test3.pkl')\n",
    "\n",
    "# df_W_Xs_test4 = pd.read_pickle('df_W_Xs_test4.pkl')\n",
    "# df_y_test4 = pd.read_pickle('df_y_test4.pkl')\n",
    "\n",
    "df_W_Xs_test5 = pd.read_pickle('df_W_Xs_test5.pkl')\n",
    "df_y_test5 = pd.read_pickle('df_y_test5.pkl')\n",
    "\n",
    "# df_W_Xs_test6 = pd.read_pickle('df_W_Xs_test6.pkl')\n",
    "# df_y_test6 = pd.read_pickle('df_y_test6.pkl')\n",
    "\n",
    "# df_W_Xs_test7 = pd.read_pickle('df_W_Xs_test7.pkl')\n",
    "# df_y_test7 = pd.read_pickle('df_y_test7.pkl')\n",
    "\n",
    "# df_W_Xs_test8 = pd.read_pickle('df_W_Xs_test8.pkl')\n",
    "# df_y_test8 = pd.read_pickle('df_y_test8.pkl')\n",
    "\n",
    "# df_W_Xs_test9 = pd.read_pickle('df_W_Xs_test9.pkl')\n",
    "# df_y_test9 = pd.read_pickle('df_y_test9.pkl')\n",
    "\n",
    "'''Prepare Numpy arrays'''\n",
    "# x_test1, y_test1 = prepare_data(sequence_length, sensors, df_W_Xs_test1, df_y_test1)\n",
    "# x_test2, y_test2 = prepare_data(sequence_length, sensors, df_W_Xs_test2, df_y_test2)\n",
    "# x_test3, y_test3 = prepare_data(sequence_length, sensors, df_W_Xs_test3, df_y_test3)\n",
    "# x_test4, y_test4 = prepare_data(sequence_length, sensors, df_W_Xs_test4, df_y_test4)\n",
    "x_test5, y_test5 = prepare_data(sequence_length, sensors, df_W_Xs_test5, df_y_test5)\n",
    "\n",
    "# x_test6, y_test6 = prepare_data(sequence_length, sensors, df_W_Xs_test6, df_y_test6)\n",
    "# x_test7, y_test7 = prepare_data(sequence_length, sensors, df_W_Xs_test7, df_y_test7)\n",
    "# x_test8, y_test8 = prepare_data(sequence_length, sensors, df_W_Xs_test8, df_y_test8)\n",
    "# x_test9, y_test9 = prepare_data(sequence_length, sensors, df_W_Xs_test9, df_y_test9)\n",
    "\n",
    "'''Save numpy arrays'''\n",
    "# np.save('RULRVE_numpydata_total/x_test1_total.npy', x_test1)\n",
    "# np.save('RULRVE_numpydata_total/y_test1_total.npy', y_test1)\n",
    "\n",
    "# np.save('RULRVE_numpydata_total/x_test2_total.npy', x_test2)\n",
    "# np.save('RULRVE_numpydata_total/y_test2_total.npy', y_test2)\n",
    "\n",
    "# np.save('RULRVE_numpydata_total/x_test3_total.npy', x_test3)\n",
    "# np.save('RULRVE_numpydata_total/y_test3_total.npy', y_test3)\n",
    "\n",
    "# np.save('RULRVE_numpydata_total/x_test4_total.npy', x_test4)\n",
    "# np.save('RULRVE_numpydata_total/y_test4_total.npy', y_test4)\n",
    "\n",
    "np.save('RULRVE_numpydata_total/x_test5_total.npy', x_test5)\n",
    "np.save('RULRVE_numpydata_total/y_test5_total.npy', y_test5)\n",
    "\n",
    "# np.save('RULRVE_numpydata_total/x_test6_total.npy', x_test6)\n",
    "# np.save('RULRVE_numpydata_total/y_test6_total.npy', y_test6)\n",
    "\n",
    "# np.save('RULRVE_numpydata_total/x_test7_total.npy', x_test7)\n",
    "# np.save('RULRVE_numpydata_total/y_test7_total.npy', y_test7)\n",
    "\n",
    "# np.save('RULRVE_numpydata_total/x_test8_total.npy', x_test8)\n",
    "# np.save('RULRVE_numpydata_total/y_test8_total.npy', y_test8)\n",
    "\n",
    "# np.save('RULRVE_numpydata_total/x_test9_total.npy', x_test9)\n",
    "# np.save('RULRVE_numpydata_total/y_test9_total.npy', y_test9)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aec22fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c24a8c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa936814",
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_train1, y_train1, x_val1, y_val1, x_test1, y_test1 = prepare_data(sequence_length, sensors, df_W_Xs_dev1, df_y_dev1, df_W_Xs_test1, df_y_test1)\n",
    "# x_train2, y_train2, x_val2, y_val2, x_test2, y_test2 = prepare_data(sequence_length, sensors, df_W_Xs_dev2, df_y_dev2, df_W_Xs_test2, df_y_test2)\n",
    "# x_train3, y_train3, x_val3, y_val3, x_test3, y_test3 = prepare_data(sequence_length, sensors, df_W_Xs_dev3, df_y_dev3, df_W_Xs_test3, df_y_test3)\n",
    "# x_train4, y_train4, x_val4, y_val4, x_test4, y_test4 = prepare_data(sequence_length, sensors, df_W_Xs_dev4, df_y_dev4, df_W_Xs_test4, df_y_test4)\n",
    "# x_train5, y_train5, x_val5, y_val5, x_test5, y_test5 = prepare_data(sequence_length, sensors, df_W_Xs_dev5, df_y_dev5, df_W_Xs_test5, df_y_test5)\n",
    "\n",
    "x_train6, y_train6, x_val6, y_val6, x_test6, y_test6 = prepare_data(sequence_length, sensors, df_W_Xs_dev6, df_y_dev6, df_W_Xs_test6, df_y_test6)\n",
    "# x_train7, y_train7, x_val7, y_val7, x_test7, y_test7 = prepare_data(sequence_length, sensors, df_W_Xs_dev7, df_y_dev7, df_W_Xs_test7, df_y_test7)\n",
    "# x_train8, y_train8, x_val8, y_val8, x_test8, y_test8 = prepare_data(sequence_length, sensors, df_W_Xs_dev8, df_y_dev8, df_W_Xs_test8, df_y_test8)\n",
    "# x_train9, y_train9, x_val9, y_val9, x_test9, y_test9 = prepare_data(sequence_length, sensors, df_W_Xs_dev9, df_y_dev9, df_W_Xs_test9, df_y_test9)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03d433c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.mkdir('./RULRVE_numpydata_total')\n",
    "\n",
    "# np.save('RULRVE_numpydata_total/x_train1.npy', x_train1)\n",
    "# np.save('RULRVE_numpydata_total/x_val1.npy', x_val1)\n",
    "# np.save('RULRVE_numpydata_total/y_train1.npy', y_train1)\n",
    "# np.save('RULRVE_numpydata_total/y_val1.npy', y_val1)\n",
    "# np.save('RULRVE_numpydata_total/x_test1.npy', x_test1)\n",
    "# y_test1.to_csv('RULRVE_numpydata_total/df_y_test1.csv')\n",
    "\n",
    "# np.save('RULRVE_numpydata_total/x_train2.npy', x_train2)\n",
    "# np.save('RULRVE_numpydata_total/x_val2.npy', x_val2)\n",
    "# np.save('RULRVE_numpydata_total/y_train2.npy', y_train2)\n",
    "# np.save('RULRVE_numpydata_total/y_val2.npy', y_val2)\n",
    "# np.save('RULRVE_numpydata_total/x_test2.npy', x_test2)\n",
    "# y_test2.to_csv('RULRVE_numpydata_total/df_y_test2.csv')\n",
    "\n",
    "# np.save('RULRVE_numpydata_total/x_train3.npy', x_train3)\n",
    "# np.save('RULRVE_numpydata_total/x_val3.npy', x_val3)\n",
    "# np.save('RULRVE_numpydata_total/y_train3.npy', y_train3)\n",
    "# np.save('RULRVE_numpydata_total/y_val3.npy', y_val3)\n",
    "# np.save('RULRVE_numpydata_total/x_test3.npy', x_test3)\n",
    "# y_test3.to_csv('RULRVE_numpydata_total/df_y_test3.csv')\n",
    "\n",
    "np.save('RULRVE_numpydata_total/x_train4.npy', x_train4)\n",
    "np.save('RULRVE_numpydata_total/x_val4.npy', x_val4)\n",
    "np.save('RULRVE_numpydata_total/y_train4.npy', y_train4)\n",
    "np.save('RULRVE_numpydata_total/y_val4.npy', y_val4)\n",
    "np.save('RULRVE_numpydata_total/x_test4.npy', x_test4)\n",
    "y_test4.to_csv('RULRVE_numpydata_total/df_y_test4.csv')\n",
    "\n",
    "# np.save('RULRVE_numpydata_total/x_train5.npy', x_train5)\n",
    "# np.save('RULRVE_numpydata_total/x_val5.npy', x_val5)\n",
    "# np.save('RULRVE_numpydata_total/y_train5.npy', y_train5)\n",
    "# np.save('RULRVE_numpydata_total/y_val5.npy', y_val5)\n",
    "# np.save('RULRVE_numpydata_total/x_test5.npy', x_test5)\n",
    "# y_test5.to_csv('RULRVE_numpydata_total/df_y_test5.csv')\n",
    "\n",
    "# np.save('RULRVE_numpydata_total/x_train6.npy', x_train6)\n",
    "# np.save('RULRVE_numpydata_total/x_val6.npy', x_val6)\n",
    "# np.save('RULRVE_numpydata_total/y_train6.npy', y_train6)\n",
    "# np.save('RULRVE_numpydata_total/y_val6.npy', y_val6)\n",
    "# np.save('RULRVE_numpydata_total/x_test6.npy', x_test6)\n",
    "# y_test6.to_csv('RULRVE_numpydata_total/df_y_test6.csv')\n",
    "\n",
    "# np.save('RULRVE_numpydata_total/x_train7.npy', x_train7)\n",
    "# np.save('RULRVE_numpydata_total/x_val7.npy', x_val7)\n",
    "# np.save('RULRVE_numpydata_total/y_train7.npy', y_train7)\n",
    "# np.save('RULRVE_numpydata_total/y_val7.npy', y_val7)\n",
    "# np.save('RULRVE_numpydata_total/x_test7.npy', x_test7)\n",
    "# y_test7.to_csv('RULRVE_numpydata_total/df_y_test7.csv')\n",
    "\n",
    "# np.save('RULRVE_numpydata_total/x_train8.npy', x_train8)\n",
    "# np.save('RULRVE_numpydata_total/x_val8.npy', x_val8)\n",
    "# np.save('RULRVE_numpydata_total/y_train8.npy', y_train8)\n",
    "# np.save('RULRVE_numpydata_total/y_val8.npy', y_val8)\n",
    "# np.save('RULRVE_numpydata_total/x_test8.npy', x_test8)\n",
    "# y_test8.to_csv('RULRVE_numpydata_total/df_y_test8.csv')\n",
    "\n",
    "# np.save('RULRVE_numpydata_total/x_train9.npy', x_train9)\n",
    "# np.save('RULRVE_numpydata_total/x_val9.npy', x_val9)\n",
    "# np.save('RULRVE_numpydata_total/y_train9.npy', y_train9)\n",
    "# np.save('RULRVE_numpydata_total/y_val9.npy', y_val9)\n",
    "# np.save('RULRVE_numpydata_total/x_test9.npy', x_test9)\n",
    "# y_test9.to_csv('RULRVE_numpydata_total/df_y_test9.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "785fa07b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_W_Xs_dev1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "424c8b09",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_W_Xs_dev2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86376f5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([df_W_Xs_dev1, df_W_Xs_dev1]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bb573e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_y_dev1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2f22b2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_y_dev2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2e16b5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([df_y_dev1, df_y_dev1]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "702cdda3",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train5 = np.load('RULRVE_numpydata_total/x_train5.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b005be7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train5.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de362b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train5[:1000, ...].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f632009",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train5.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f5b3fc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array_split(x_train5, 100)[-1], "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73584d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train3 = np.load('RULRVE_numpydata_total/x_train3.npy')\n",
    "y_train3 = np.load('RULRVE_numpydata_total/y_train3.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "536e1288",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = tf.data.Dataset.from_tensor_slices((x_train3, y_train3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac2d02c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(list(a.as_numpy_iterator()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3856a0c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loader(filepath):\n",
    "    with open(filepath, 'rb') as fp:\n",
    "        np_ndarray = np.load(fp)\n",
    "    print(\"{} has a numpy ndarray of shape: {}\".format(filepath, np_ndarray.shape))\n",
    "    return np_ndarray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57fa6847",
   "metadata": {},
   "outputs": [],
   "source": [
    "from os.path import join as opj\n",
    "data_dir = '/data/adas_vision_data1/users/adithya/turbofan_ncmapss/RULRVE_numpydata_total'\n",
    "x_test = None\n",
    "for i in [1]:#,2,3]:\n",
    "    x_test = loader(opj(data_dir, 'x_test{}.npy'.format(i)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c13a103",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e965de30",
   "metadata": {},
   "outputs": [],
   "source": [
    "for j,array in enumerate(np.array_split(x_test, 1)):\n",
    "    # print(\"{}/100\".format(j))\n",
    "    print(array.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c5991c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_y_test = pd.read_csv(opj(data_dir, 'df_y_test1.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4952e7de",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95af0cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = df_y_test.RUL.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25ed9a0a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "013ea329",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "y_test.reshape(len(y_test), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2faefe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = loader(opj(data_dir, 'x_test2.npy'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68dd9d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79119742",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_val = loader(opj(data_dir, 'y_val1.npy'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ef17268",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_val = loader(opj(data_dir, 'x_val1.npy'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2455de6",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f37cfdf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i ,j in np.array_split(x_val, 100), y_val"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a08104f",
   "metadata": {},
   "source": [
    "# Saving the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3868faf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/home/a0484689/PycharmProjects/Remaining-Useful-Life-Estimation-Variational')\n",
    "import model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cec8440e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "timesteps = 30 # x_train1.shape[1]\n",
    "input_dim = 15 # x_train2.shape[2]\n",
    "intermediate_dim = 300\n",
    "batch_size = 1024 # 1024\n",
    "latent_dim = 2\n",
    "epochs = 1000 # 1000\n",
    "optimizer = 'adam'\n",
    "masking_value = -99\n",
    "# RVE = model.create_model(timesteps, input_dim, intermediate_dim, batch_size, latent_dim, epochs, optimizer, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8cf70a59",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from os.path import join as opj\n",
    "data_dir = \"/data/adas_vision_data1/users/adithya/turbofan_ncmapss/RULRVE_numpydata_scriptgen\"\n",
    "def numpy_loader(filepath):\n",
    "    logger = logging.getLogger(\"root.numpy_loader\")\n",
    "    with open(filepath, 'rb') as fp:\n",
    "        np_ndarray = np.load(fp)\n",
    "    logger.info(\"{} has a numpy ndarray of shape: {}\".format(filepath, np_ndarray.shape))\n",
    "    return np_ndarray\n",
    "\n",
    "filename_suffix = \"DS01-005\"\n",
    "x_test = numpy_loader(opj(data_dir, 'x_test_{}.npy'.format(filename_suffix)))[::100, ...]\n",
    "y_test = numpy_loader(opj(data_dir, 'y_test_{}.npy'.format(filename_suffix)))[::100, ...]\n",
    "# test_mu = np.array([], dtype=np.float32).reshape(0, 2)\n",
    "# y_test = y_test.reshape(len(y_test), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ea747d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------- Encoder -----------------------\n",
    "inputs = Input(shape=(timesteps, input_dim,), name='encoder_input', )#batch_size=batch_size)\n",
    "\n",
    "mask = Masking(mask_value=masking_value)(inputs)\n",
    "\n",
    "# LSTM encoding\n",
    "h = Bidirectional(LSTM(intermediate_dim))(mask) \n",
    "\n",
    "# VAE Z layer\n",
    "mu = Dense(latent_dim)(h)\n",
    "sigma = Dense(latent_dim)(h)\n",
    "# z = Sampling()([mu, sigma])\n",
    "epsilon = tf.keras.backend.random_normal(shape=(tf.shape(mu)[0], tf.shape(mu)[1]))\n",
    "z = mu + tf.exp(0.5 * sigma) * epsilon\n",
    "\n",
    "# Instantiate the encoder model:\n",
    "encoder = keras.Model(inputs, [mu, sigma, z], name='encoder')\n",
    "print(encoder.summary())\n",
    "# -------------------------------------------------------\n",
    "\n",
    "\n",
    "# ----------------------- Regressor --------------------\n",
    "reg_latent_inputs = Input(shape=(latent_dim,), name='z_sampling_reg')\n",
    "reg_intermediate = Dense(200, activation='tanh')(reg_latent_inputs)\n",
    "reg_outputs = Dense(1, name='reg_output')(reg_intermediate)\n",
    "# Instantiate the classifier model:\n",
    "regressor = keras.Model(reg_latent_inputs, reg_outputs, name='regressor')\n",
    "print(regressor.summary())\n",
    "# -------------------------------------------------------\n",
    "\n",
    "\n",
    "class RVE(keras.Model):\n",
    "    def __init__(self, encoder, regressor, decoder=None, **kwargs):\n",
    "        super(RVE, self).__init__(**kwargs)\n",
    "        self.encoder = encoder\n",
    "        self.regressor = regressor\n",
    "        self.total_loss_tracker = keras.metrics.Mean(name=\"total_loss\")\n",
    "        self.kl_loss_tracker = keras.metrics.Mean(name=\"kl_loss\")\n",
    "        self.reg_loss_tracker = keras.metrics.Mean(name=\"reg_loss\")\n",
    "        self.decoder = decoder\n",
    "        if self.decoder!=None:\n",
    "            self.reconstruction_loss_tracker = keras.metrics.Mean(name=\"reconstruction_loss\")\n",
    "            \n",
    "    def __call__(self, inputs, **kwargs):\n",
    "        x = inputs\n",
    "        z, _, _ = self.encoder(x)#(x, y)\n",
    "        return self.regressor(z)\n",
    "        \n",
    "        \n",
    "\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        if self.decoder!=None:\n",
    "            return [\n",
    "                self.total_loss_tracker,\n",
    "                self.kl_loss_tracker,\n",
    "                self.reg_loss_tracker,\n",
    "                self.reconstruction_loss_tracker\n",
    "            ]\n",
    "        else:\n",
    "            return [\n",
    "                self.total_loss_tracker,\n",
    "                self.kl_loss_tracker,\n",
    "                self.reg_loss_tracker,\n",
    "            ]\n",
    "\n",
    "    def train_step(self, data):\n",
    "        x, target_x = data\n",
    "        with tf.GradientTape() as tape:\n",
    "            # kl loss\n",
    "            mu, sigma, z = self.encoder(x)\n",
    "            kl_loss = -0.5 * (1 + sigma - tf.square(mu) - tf.exp(sigma))\n",
    "            kl_loss = tf.reduce_mean(tf.reduce_sum(kl_loss, axis=1))\n",
    "            # Regressor\n",
    "            reg_prediction = self.regressor(z)\n",
    "            reg_loss = tf.reduce_mean(\n",
    "                keras.losses.mse(target_x, reg_prediction)\n",
    "            )\n",
    "            # Reconstruction\n",
    "            if self.decoder!=None:\n",
    "                reconstruction = self.decoder(z)\n",
    "                reconstruction_loss = tf.reduce_mean(keras.losses.mse(x, reconstruction))\n",
    "                total_loss = kl_loss + reg_loss + reconstruction_loss\n",
    "                self.reconstruction_loss_tracker.update_state(reconstruction_loss)\n",
    "            else:\n",
    "                total_loss = kl_loss + reg_loss\n",
    "        grads = tape.gradient(total_loss, self.trainable_weights)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.trainable_weights))\n",
    "        self.total_loss_tracker.update_state(total_loss)\n",
    "        self.kl_loss_tracker.update_state(kl_loss)\n",
    "        self.reg_loss_tracker.update_state(reg_loss)\n",
    "        return {\n",
    "            \"loss\": self.total_loss_tracker.result(),\n",
    "            \"kl_loss\": self.kl_loss_tracker.result(),\n",
    "            \"reg_loss\": self.reg_loss_tracker.result(),\n",
    "        }\n",
    "\n",
    "\n",
    "    def test_step(self, data):\n",
    "        x, target_x = data\n",
    "\n",
    "        # kl loss\n",
    "        mu, sigma, z = self.encoder(x)\n",
    "        kl_loss = -0.5 * (1 + sigma - tf.square(mu) - tf.exp(sigma))\n",
    "        kl_loss = tf.reduce_mean(tf.reduce_sum(kl_loss, axis=1))\n",
    "        # Regressor\n",
    "        reg_prediction = self.regressor(z)\n",
    "        reg_loss = tf.reduce_mean(\n",
    "            keras.losses.mse(target_x, reg_prediction)\n",
    "        )\n",
    "        # Reconstruction\n",
    "        if self.decoder!=None:\n",
    "            reconstruction = self.decoder(z)\n",
    "            reconstruction_loss = tf.reduce_mean(keras.losses.mse(x, reconstruction))\n",
    "\n",
    "            total_loss = kl_loss + reg_loss + reconstruction_loss\n",
    "        else:\n",
    "            total_loss = kl_loss + reg_loss\n",
    "\n",
    "        return {\n",
    "            \"loss\": total_loss,\n",
    "            \"kl_loss\": kl_loss,\n",
    "            \"reg_loss\": reg_loss,\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccd8b185",
   "metadata": {},
   "source": [
    "RVE = model.create_model(timesteps, input_dim, intermediate_dim,\n",
    "                                 batch_size, latent_dim, epochs, optimizer, )\n",
    "RVE.load_weights(tf.train.latest_checkpoint(\"/home/a0484689/PycharmProjects/Remaining-Useful-Life-Estimation-Variational/checkpoints_all_datasets/\"))\n",
    "\n",
    "# z, _, _ = RVE.encoder.predict(x_test[::1, ...])\n",
    "# RVE.regressor.predict(z)\n",
    "\n",
    "RVE(x_test[::100, ...])\n",
    "\n",
    "from keras.models import Model\n",
    "newInput = Input(batch_shape=(1,30,15))\n",
    "newOutputs = RVE(newInput)\n",
    "newModel = Model(newInput,newOutputs)\n",
    "\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(newModel)\n",
    "tfmodel = converter.convert()\n",
    "\n",
    "open (\"RVE_NCMAPSS_all_datasets_org.tflite\" , \"wb\") .write(tfmodel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc994348",
   "metadata": {},
   "outputs": [],
   "source": [
    "RVE = RVE(encoder, regressor)\n",
    "RVE.compile(optimizer=optimizer)\n",
    "RVE.load_weights(tf.train.latest_checkpoint(\"/home/a0484689/PycharmProjects/Remaining-Useful-Life-Estimation-Variational/checkpoints_all_datasets/\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f664df74",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d4661b0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_pred = RVE(x_test[::1, ...])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c369a950",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"RMSE Score: {}\".format(np.sqrt(mean_squared_error(y_test[::1, ...], y_pred))))\n",
    "print(\"NASA Score: {}\".format(utils.score(y_test[::1, ...], y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50470528",
   "metadata": {},
   "source": [
    "converter = tf.lite.TFLiteConverter.from_keras_model(RVE.encoder)\n",
    "tfmodel = converter.convert()\n",
    "open (\"RVE_NCMAPSS_DS01_3hidden_layers_unchanged_input_shape_encoder.tflite\" , \"wb\") .write(tfmodel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aefbb38b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "newInput = Input(batch_shape=(1,30,15))\n",
    "newOutputs = RVE(newInput)\n",
    "newModel = Model(newInput,newOutputs)\n",
    "\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(newModel)\n",
    "tfmodel = converter.convert()\n",
    "open (\"RVE_NCMAPSS_DS01_3hidden_layers_unchanged_input_shape.tflite\" , \"wb\") .write(tfmodel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5972f1bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "newOutputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18d4e322",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
